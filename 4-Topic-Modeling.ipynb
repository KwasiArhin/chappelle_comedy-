{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>aand</th>\n",
       "      <th>abandon</th>\n",
       "      <th>able</th>\n",
       "      <th>ablebodied</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zeke</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zigzagging</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippitydoodah</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_spin</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep_texas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equanimity</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killin_softly</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick_stones</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 4435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aaaah  aaah  aah  aand  abandon  able  ablebodied  \\\n",
       "age_spin                 0     1    0     0        0     0           0   \n",
       "deep_texas               0     0    0     0        0     0           0   \n",
       "equanimity               0     0    0     0        0     1           0   \n",
       "killin_softly            0     0    0     0        0     1           0   \n",
       "stick_stones             1     0    2     2        1     1           1   \n",
       "the_bird_revelation      0     0    0     0        0     1           0   \n",
       "worth                    0     0    0     0        0     1           0   \n",
       "\n",
       "                     abomination  abortion  abortions  ...  youtube  youve  \\\n",
       "age_spin                       0         0          1  ...        0      5   \n",
       "deep_texas                     0         0          0  ...        2      2   \n",
       "equanimity                     0         0          0  ...        0      1   \n",
       "killin_softly                  0         0          0  ...        0      2   \n",
       "stick_stones                   1         1          0  ...        0      2   \n",
       "the_bird_revelation            0         0          0  ...        0      1   \n",
       "worth                          0         0          0  ...        0      2   \n",
       "\n",
       "                     yuck  zeke  zero  zigzag  zigzagging  zip  zippitydoodah  \\\n",
       "age_spin                0     0     0       0           0    0              0   \n",
       "deep_texas              0     0     1       0           0    0              0   \n",
       "equanimity              2     1     0       0           0    0              0   \n",
       "killin_softly           0     0     0       0           1    2              1   \n",
       "stick_stones            0     0     0       1           0    0              0   \n",
       "the_bird_revelation     4     0     0       0           0    0              0   \n",
       "worth                   0     0     0       0           0    0              0   \n",
       "\n",
       "                     zone  \n",
       "age_spin                0  \n",
       "deep_texas              1  \n",
       "equanimity              0  \n",
       "killin_softly           0  \n",
       "stick_stones            0  \n",
       "the_bird_revelation     0  \n",
       "worth                   0  \n",
       "\n",
       "[7 rows x 4435 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_spin</th>\n",
       "      <th>deep_texas</th>\n",
       "      <th>equanimity</th>\n",
       "      <th>killin_softly</th>\n",
       "      <th>stick_stones</th>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <th>worth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age_spin  deep_texas  equanimity  killin_softly  stick_stones  \\\n",
       "aaaah           0           0           0              0             1   \n",
       "aaah            1           0           0              0             0   \n",
       "aah             0           0           0              0             2   \n",
       "aand            0           0           0              0             2   \n",
       "abandon         0           0           0              0             1   \n",
       "\n",
       "         the_bird_revelation  worth  \n",
       "aaaah                      0      0  \n",
       "aaah                       0      0  \n",
       "aah                        0      0  \n",
       "aand                       0      0  \n",
       "abandon                    0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"time\" + 0.007*\"ahah\" + 0.006*\"black\" + 0.006*\"good\" + 0.006*\"fucking\" + 0.005*\"want\" + 0.005*\"oh\" + 0.005*\"hes\" + 0.005*\"did\" + 0.005*\"youre\"'),\n",
       " (1,\n",
       "  '0.006*\"white\" + 0.006*\"oh\" + 0.006*\"nigga\" + 0.005*\"black\" + 0.005*\"time\" + 0.005*\"mean\" + 0.005*\"youre\" + 0.005*\"motherfucker\" + 0.005*\"did\" + 0.005*\"think\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"white\" + 0.006*\"black\" + 0.006*\"oh\" + 0.006*\"time\" + 0.006*\"youre\" + 0.005*\"motherfucker\" + 0.005*\"think\" + 0.005*\"fucking\" + 0.005*\"cause\" + 0.005*\"come\"'),\n",
       " (1,\n",
       "  '0.007*\"nigga\" + 0.006*\"uh\" + 0.005*\"white\" + 0.005*\"mean\" + 0.005*\"did\" + 0.005*\"school\" + 0.004*\"oh\" + 0.004*\"going\" + 0.004*\"everybody\" + 0.004*\"want\"'),\n",
       " (2,\n",
       "  '0.008*\"ahah\" + 0.007*\"time\" + 0.007*\"black\" + 0.007*\"good\" + 0.006*\"fucking\" + 0.006*\"want\" + 0.006*\"hes\" + 0.006*\"did\" + 0.005*\"oh\" + 0.005*\"nigga\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"time\" + 0.007*\"black\" + 0.007*\"good\" + 0.006*\"nigga\" + 0.006*\"fucking\" + 0.006*\"did\" + 0.006*\"hes\" + 0.005*\"want\" + 0.005*\"ahah\" + 0.005*\"oh\"'),\n",
       " (1,\n",
       "  '0.007*\"oh\" + 0.007*\"black\" + 0.006*\"white\" + 0.006*\"time\" + 0.006*\"youre\" + 0.006*\"fucking\" + 0.006*\"think\" + 0.006*\"good\" + 0.005*\"motherfucker\" + 0.005*\"come\"'),\n",
       " (2,\n",
       "  '0.000*\"black\" + 0.000*\"good\" + 0.000*\"white\" + 0.000*\"oh\" + 0.000*\"did\" + 0.000*\"come\" + 0.000*\"want\" + 0.000*\"mean\" + 0.000*\"cause\" + 0.000*\"everybody\"'),\n",
       " (3,\n",
       "  '0.007*\"nigga\" + 0.006*\"uh\" + 0.005*\"white\" + 0.005*\"mean\" + 0.005*\"school\" + 0.005*\"did\" + 0.005*\"oh\" + 0.004*\"going\" + 0.004*\"everybody\" + 0.004*\"okay\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_spin</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep_texas</th>\n",
       "      <td>hes in the trance he isnt thinking of jokes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equanimity</th>\n",
       "      <td>equanimity was shot in washington dc and it co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killin_softly</th>\n",
       "      <td>wooo yall gone make me lose my mind up in here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick_stones</th>\n",
       "      <td>sticks  stones is dave chappelles fifth netfli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <td>recorded at the comedy store in los angeles in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>whyd you pick san francisco to shoot your spec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            transcript\n",
       "age_spin             this is dave he tells dirty jokes for a living...\n",
       "deep_texas            hes in the trance he isnt thinking of jokes t...\n",
       "equanimity           equanimity was shot in washington dc and it co...\n",
       "killin_softly        wooo yall gone make me lose my mind up in here...\n",
       "stick_stones         sticks  stones is dave chappelles fifth netfli...\n",
       "the_bird_revelation  recorded at the comedy store in los angeles in...\n",
       "worth                whyd you pick san francisco to shoot your spec..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_spin</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep_texas</th>\n",
       "      <td>hes trance thinking jokes hes voiceover im dre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equanimity</th>\n",
       "      <td>equanimity washington dc material chappelle st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killin_softly</th>\n",
       "      <td>wooo yall mind fool cool lincoln theater washi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick_stones</th>\n",
       "      <td>sticks stones chappelles trailer morgan chappe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <td>comedy store angeles november thing position l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>whyd francisco towns comedy venue comedians br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            transcript\n",
       "age_spin             jokes living stare work profound train thought...\n",
       "deep_texas           hes trance thinking jokes hes voiceover im dre...\n",
       "equanimity           equanimity washington dc material chappelle st...\n",
       "killin_softly        wooo yall mind fool cool lincoln theater washi...\n",
       "stick_stones         sticks stones chappelles trailer morgan chappe...\n",
       "the_bird_revelation  comedy store angeles november thing position l...\n",
       "worth                whyd francisco towns comedy venue comedians br..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>aand</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>absolutism</th>\n",
       "      <th>abuse</th>\n",
       "      <th>academy</th>\n",
       "      <th>accident</th>\n",
       "      <th>accountability</th>\n",
       "      <th>...</th>\n",
       "      <th>youthis</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zigzagging</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippitydoodah</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_spin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep_texas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equanimity</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killin_softly</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick_stones</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 2678 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aah  aand  abomination  abortion  abortions  absolutism  \\\n",
       "age_spin               0     0            0         0          1           0   \n",
       "deep_texas             0     0            0         0          0           0   \n",
       "equanimity             0     0            0         0          0           0   \n",
       "killin_softly          0     0            0         0          0           0   \n",
       "stick_stones           2     1            1         1          0           0   \n",
       "the_bird_revelation    0     0            0         0          0           1   \n",
       "worth                  0     0            0         0          0           0   \n",
       "\n",
       "                     abuse  academy  accident  accountability  ...  youthis  \\\n",
       "age_spin                 0        0         0               0  ...        1   \n",
       "deep_texas               0        0         0               0  ...        0   \n",
       "equanimity               0        0         0               0  ...        0   \n",
       "killin_softly            0        0         0               0  ...        0   \n",
       "stick_stones             0        1         1               0  ...        0   \n",
       "the_bird_revelation      0        0         1               1  ...        0   \n",
       "worth                    1        0         1               0  ...        0   \n",
       "\n",
       "                     youtube  youve  yuck  zero  zigzag  zigzagging  zip  \\\n",
       "age_spin                   0      1     0     0       0           0    0   \n",
       "deep_texas                 2      0     0     1       0           0    0   \n",
       "equanimity                 0      0     1     0       0           0    0   \n",
       "killin_softly              0      1     0     0       0           1    1   \n",
       "stick_stones               0      0     0     0       1           0    0   \n",
       "the_bird_revelation        0      1     1     0       0           0    0   \n",
       "worth                      0      1     0     0       0           0    0   \n",
       "\n",
       "                     zippitydoodah  zone  \n",
       "age_spin                         0     0  \n",
       "deep_texas                       0     1  \n",
       "equanimity                       0     0  \n",
       "killin_softly                    1     0  \n",
       "stick_stones                     0     0  \n",
       "the_bird_revelation              0     0  \n",
       "worth                            0     0  \n",
       "\n",
       "[7 rows x 2678 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"shit\" + 0.014*\"man\" + 0.010*\"fuck\" + 0.008*\"motherfucker\" + 0.007*\"hes\" + 0.007*\"guy\" + 0.007*\"years\" + 0.007*\"bitch\" + 0.006*\"gon\" + 0.006*\"everybody\"'),\n",
       " (1,\n",
       "  '0.021*\"man\" + 0.018*\"shit\" + 0.008*\"fuck\" + 0.007*\"motherfucker\" + 0.007*\"guy\" + 0.006*\"thing\" + 0.006*\"women\" + 0.006*\"baby\" + 0.006*\"house\" + 0.006*\"gon\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"shit\" + 0.017*\"man\" + 0.011*\"motherfucker\" + 0.010*\"fuck\" + 0.006*\"everybody\" + 0.006*\"gon\" + 0.006*\"lot\" + 0.005*\"night\" + 0.005*\"didnt\" + 0.005*\"cause\"'),\n",
       " (1,\n",
       "  '0.020*\"man\" + 0.018*\"shit\" + 0.011*\"guy\" + 0.009*\"fuck\" + 0.008*\"bitch\" + 0.008*\"hes\" + 0.007*\"thing\" + 0.007*\"gon\" + 0.006*\"way\" + 0.006*\"women\"'),\n",
       " (2,\n",
       "  '0.011*\"shit\" + 0.009*\"man\" + 0.008*\"school\" + 0.008*\"fuck\" + 0.006*\"way\" + 0.006*\"everybody\" + 0.006*\"years\" + 0.006*\"hes\" + 0.006*\"kids\" + 0.006*\"gon\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"shit\" + 0.018*\"man\" + 0.011*\"fuck\" + 0.009*\"guy\" + 0.008*\"gon\" + 0.007*\"way\" + 0.007*\"motherfucker\" + 0.007*\"everybody\" + 0.006*\"hes\" + 0.006*\"cause\"'),\n",
       " (1,\n",
       "  '0.017*\"shit\" + 0.015*\"man\" + 0.009*\"fuck\" + 0.009*\"money\" + 0.009*\"bitch\" + 0.009*\"lot\" + 0.009*\"hes\" + 0.007*\"women\" + 0.007*\"motherfucker\" + 0.006*\"didnt\"'),\n",
       " (2,\n",
       "  '0.021*\"shit\" + 0.019*\"man\" + 0.013*\"motherfucker\" + 0.009*\"fuck\" + 0.006*\"kids\" + 0.006*\"everybody\" + 0.006*\"kid\" + 0.006*\"nigger\" + 0.005*\"gon\" + 0.005*\"balls\"'),\n",
       " (3,\n",
       "  '0.001*\"shit\" + 0.001*\"man\" + 0.001*\"fuck\" + 0.001*\"motherfucker\" + 0.001*\"guy\" + 0.001*\"thing\" + 0.001*\"way\" + 0.001*\"gon\" + 0.001*\"hes\" + 0.001*\"everybody\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_spin</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep_texas</th>\n",
       "      <td>hes trance thinking jokes hes voiceover im wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equanimity</th>\n",
       "      <td>equanimity washington dc material chappelle mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killin_softly</th>\n",
       "      <td>wooo yall mind yall yall fool yall cool histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick_stones</th>\n",
       "      <td>sticks stones dave chappelles promotional trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <td>comedy store los angeles november funniest thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>whyd san francisco special best towns comedy h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            transcript\n",
       "age_spin             dirty jokes living stare most hard work profou...\n",
       "deep_texas           hes trance thinking jokes hes voiceover im wil...\n",
       "equanimity           equanimity washington dc material chappelle mo...\n",
       "killin_softly        wooo yall mind yall yall fool yall cool histor...\n",
       "stick_stones         sticks stones dave chappelles promotional trai...\n",
       "the_bird_revelation  comedy store los angeles november funniest thi...\n",
       "worth                whyd san francisco special best towns comedy h..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>aand</th>\n",
       "      <th>abandon</th>\n",
       "      <th>able</th>\n",
       "      <th>ablebodied</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abraham</th>\n",
       "      <th>absolutism</th>\n",
       "      <th>...</th>\n",
       "      <th>youthis</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuck</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zigzagging</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippitydoodah</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_spin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep_texas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equanimity</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killin_softly</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick_stones</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the_bird_revelation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 3133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aah  aand  abandon  able  ablebodied  abomination  \\\n",
       "age_spin               0     0        0     0           0            0   \n",
       "deep_texas             0     0        0     0           0            0   \n",
       "equanimity             0     0        0     1           0            0   \n",
       "killin_softly          0     0        0     1           0            0   \n",
       "stick_stones           2     1        1     1           1            1   \n",
       "the_bird_revelation    0     0        0     1           0            0   \n",
       "worth                  0     0        0     1           0            0   \n",
       "\n",
       "                     abortion  abortions  abraham  absolutism  ...  youthis  \\\n",
       "age_spin                    0          1        0           0  ...        1   \n",
       "deep_texas                  0          0        0           0  ...        0   \n",
       "equanimity                  0          0        1           0  ...        0   \n",
       "killin_softly               0          0        0           0  ...        0   \n",
       "stick_stones                1          0        0           0  ...        0   \n",
       "the_bird_revelation         0          0        0           1  ...        0   \n",
       "worth                       0          0        0           0  ...        0   \n",
       "\n",
       "                     youtube  youve  yuck  zero  zigzag  zigzagging  zip  \\\n",
       "age_spin                   0      2     0     0       0           0    0   \n",
       "deep_texas                 2      0     0     1       0           0    0   \n",
       "equanimity                 0      1     1     0       0           0    0   \n",
       "killin_softly              0      1     0     0       0           1    1   \n",
       "stick_stones               0      0     0     0       1           0    0   \n",
       "the_bird_revelation        0      1     2     0       0           0    0   \n",
       "worth                      0      1     0     0       0           0    0   \n",
       "\n",
       "                     zippitydoodah  zone  \n",
       "age_spin                         0     0  \n",
       "deep_texas                       0     1  \n",
       "equanimity                       0     0  \n",
       "killin_softly                    1     0  \n",
       "stick_stones                     0     0  \n",
       "the_bird_revelation              0     0  \n",
       "worth                            0     0  \n",
       "\n",
       "[7 rows x 3133 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"gay\" + 0.006*\"ahah\" + 0.004*\"peace\" + 0.003*\"michael\" + 0.003*\"nigger\" + 0.003*\"girl\" + 0.003*\"juice\" + 0.003*\"war\" + 0.003*\"oj\" + 0.003*\"kid\"'),\n",
       " (1,\n",
       "  '0.005*\"police\" + 0.004*\"uh\" + 0.004*\"tape\" + 0.004*\"gay\" + 0.004*\"president\" + 0.004*\"poor\" + 0.003*\"yo\" + 0.003*\"jokes\" + 0.003*\"banana\" + 0.003*\"hot\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"president\" + 0.006*\"police\" + 0.006*\"chip\" + 0.004*\"girl\" + 0.004*\"chicken\" + 0.004*\"dc\" + 0.004*\"uh\" + 0.004*\"famous\" + 0.004*\"crack\" + 0.004*\"clinton\"'),\n",
       " (1,\n",
       "  '0.005*\"poor\" + 0.005*\"uh\" + 0.005*\"yo\" + 0.004*\"gay\" + 0.004*\"niggas\" + 0.004*\"jokes\" + 0.004*\"trump\" + 0.004*\"peace\" + 0.003*\"iceberg\" + 0.003*\"em\"'),\n",
       " (2,\n",
       "  '0.009*\"ahah\" + 0.008*\"gay\" + 0.005*\"nigger\" + 0.004*\"ghetto\" + 0.004*\"tape\" + 0.004*\"juice\" + 0.004*\"girl\" + 0.004*\"banana\" + 0.003*\"police\" + 0.003*\"water\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"ahah\" + 0.000*\"gay\" + 0.000*\"banana\" + 0.000*\"tape\" + 0.000*\"girl\" + 0.000*\"ghetto\" + 0.000*\"police\" + 0.000*\"sandwich\" + 0.000*\"ray\" + 0.000*\"crowd\"'),\n",
       " (1,\n",
       "  '0.005*\"poor\" + 0.005*\"uh\" + 0.005*\"kid\" + 0.004*\"nigger\" + 0.004*\"michael\" + 0.004*\"yo\" + 0.004*\"jackson\" + 0.003*\"wan\" + 0.003*\"trump\" + 0.003*\"jokes\"'),\n",
       " (2,\n",
       "  '0.009*\"ahah\" + 0.009*\"gay\" + 0.005*\"peace\" + 0.005*\"iceberg\" + 0.004*\"oj\" + 0.004*\"ghetto\" + 0.004*\"war\" + 0.004*\"kevin\" + 0.004*\"rumors\" + 0.004*\"niggas\"'),\n",
       " (3,\n",
       "  '0.007*\"police\" + 0.006*\"tape\" + 0.005*\"banana\" + 0.005*\"gay\" + 0.005*\"girl\" + 0.005*\"president\" + 0.005*\"ahah\" + 0.004*\"chicken\" + 0.004*\"chip\" + 0.004*\"ghetto\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"president\" + 0.005*\"trump\" + 0.005*\"letter\" + 0.005*\"police\" + 0.004*\"yo\" + 0.004*\"dc\" + 0.004*\"chip\" + 0.003*\"better\" + 0.003*\"group\" + 0.003*\"hello\"'),\n",
       " (1,\n",
       "  '0.008*\"gay\" + 0.006*\"tape\" + 0.006*\"banana\" + 0.005*\"peace\" + 0.005*\"iceberg\" + 0.005*\"ahah\" + 0.004*\"rumors\" + 0.004*\"girl\" + 0.004*\"louis\" + 0.004*\"york\"'),\n",
       " (2,\n",
       "  '0.000*\"trump\" + 0.000*\"yo\" + 0.000*\"letter\" + 0.000*\"poor\" + 0.000*\"niggas\" + 0.000*\"media\" + 0.000*\"jokes\" + 0.000*\"paper\" + 0.000*\"joke\" + 0.000*\"problem\"'),\n",
       " (3,\n",
       "  '0.014*\"ahah\" + 0.010*\"gay\" + 0.006*\"oj\" + 0.006*\"ghetto\" + 0.005*\"kevin\" + 0.004*\"water\" + 0.004*\"juice\" + 0.004*\"simpson\" + 0.003*\"hollywood\" + 0.003*\"asian\"'),\n",
       " (4,\n",
       "  '0.007*\"uh\" + 0.006*\"nigger\" + 0.006*\"kid\" + 0.006*\"michael\" + 0.005*\"jackson\" + 0.005*\"poor\" + 0.005*\"gs\" + 0.004*\"wan\" + 0.004*\"gay\" + 0.004*\"balls\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: government,policing\n",
    "* Topic 1: difficult to decipher \n",
    "* Topic 2: trump and media \n",
    "* Topic 3: Hollywood-OJ Simpson \n",
    "* Topic 4: Michael Jackson, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'age_spin'),\n",
       " (1, 'deep_texas'),\n",
       " (0, 'equanimity'),\n",
       " (0, 'killin_softly'),\n",
       " (4, 'stick_stones'),\n",
       " (1, 'the_bird_revelation'),\n",
       " (4, 'worth')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Could use this accompanied with watching the specials to really nailing down topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
